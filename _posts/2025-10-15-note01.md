---
title: Pytorch深度学习实践
categories: [AI笔记]
tags: [CS,AI]
pin: false # 置顶文章
toc: true # 目录
comments: false # 评论
---

## 1 OVERVIEW
### 深度学习
- 机器学习以监督学习为主，深度学习是机器学习的分支。
   表示学习：提取特征--浅层与深层。其中有很多实现方式
   deeplearning：模型是神经网络，目标是表示学习
- Classic machine learning:Input--人工特征提取--寻找y=f(x)的函数
- **维度诅咒**：维度越多，数据需求越大--Features降维
   ![alt text](/assets/img/2025-10-15-note01-01.png)
Features(训练特征提取器)-->学习器
DEEPLEARNING:end to end
Input->训练整个模型-->output
- 传统的机器学习策略

### 神经网络
- 仿真方式（感知机），输入后加权求解
- 反向传播--求偏导。核心：计算图
- ***原子计算符***反向传播算法！（有点晕）
  多条路径：e对b而言，就是每一条路的偏导之和，每一条路径的偏导就是每一步的偏导之积
  ![alt text](/assets/img/image.png)
- 发展
  LeNet-5-->AlexNet-->GoogleNet VGG-->ResNet
  关键是了解基本块的实现！然后进行组装
- 流行
  Theano  Tensorflow
  Caffe
  Torch

## 2 线性模型
### 步骤
  1. Dataset
  2. Model
  3. Training(权重训练)
  4. inferring
### 举例
#### dataset
  测试集结果已知但是暂时不能使用。
  数据采样可能不能完全模拟结果-->数据集要和真实数据集尽可能一致。
  --**过拟合**-->希望泛化能力强：对没有见过的图片的准确度也比较高
  故：训练集，开发集（评估性能），测试集
  简化：training set +test set
#### 目标：找到y=f(x)
常见思路:
- 先使用线性模型尝试，效果差再调整（Linear Model:F(x)=w*x+b）
- 如何找到最合适的w和b，简化为y_hat=w*b
   1. 随机猜测一个w,求出`|y_hat(i)-y(i)|**2=loss`
   也即loss:![alt text](/assets/img/image-1.png)
   求出loss的mean，Mean Square Error,**MSE**(平均平方误差)
   可用穷举法得到损失函数，找到最低点就是最佳w

   ```python
   import numpy as np
   import matplotlib.pyplot as plt

   x_data=[1,2,3]
   y_data=[2,4,6]

   def forward(x):#向前函数
    return x*w

   def loss(x,y):#计算损失函数
    y_pred=forward(x)
    return (y_pred-y)*(y_pred-y)
   
   w_list=[]
   mes_list=[]
   for w in np.arange(0.0,4.1,0.1):#生成从0.0到4.1间隔为0.1的数列
      print('w=',w)
      loss_sum=0
      for x_val,y_val in zip(x_data,y_data):#解包x_data和y_data配对迭代器
        y_pred_val=forward(x_val)
        loss_val=loss(x_val,y_val)
        loss_sum+=loss_val
        print('\t',x_val,y_val,y_pred_val,loss_val)

   ```

## 3 梯度下降算法实践(Gradient Descent)

### 梯度下降算法解析
cost对w求导，若导数>0则cost随w增加而增加，故需要沿着负方向，
w=w-a*(dcost/dw)：a是学习率，-代表更新w是朝着cost减小的方向（**贪心算法**局部最优）
没有全局最优，只有局部最优，但实际上损失函数中局部最优点极少
但存在**鞍点**：梯度为0，无法更新w
注意此处的局部求导过程，需要掌握

### 代码实现


```python
import numpy as np
import matplotlib.pyplot as plt
x_data=[1.0,2.0,3.0]
y_data=[2.0,4.0,6.0]

w=1.0

def forward(x):
    return x*w
    
def cost(xs,ys):#计算损失
    cost=0
    for x,y in zip(xs,ys):
        y_pred=forward(x)
        cost+=(y_pred-y)**2
        
    return cost/len(xs)
def gradient(xs,ys):#计算梯度
    grad=0
    for x,y in zip(xs,ys):
        grad+=2*x*(x*w-y)#数学推导得到的结果
    return grad/len(xs)
cost_n=[]
epoch_=[]
print(4,forward(4))
for epoch in range(100):
    cost_val=cost(x_data,y_data)
    grad_val=gradient(x_data,y_data)
    w-=0.01*grad_val  #学习率为0.01
    cost_n.append(cost_val)
    epoch_.append(epoch)
print(4,forward(4))
plt.plot(epoch_,cost_n)
plt.ylabel('cost')
plt.xlabel('eposch')
plt.show()
```

    4 4.0
    4 7.999777758621207
    


    
![alt text](/assets/img/output_3_1.png)
    


如果：cost上升，说明训练失败，则一般需要减小学习率

多种梯度下降更新方式：
**随机梯度下降SGD**（引入随机噪声，解决鞍点问题，可能可以跨越鞍点）
-->对每一个样本梯度进行更新
优点：性能比较好
缺点：不能并行使用w，只能依次取w，时间复杂度较高


```python
import numpy as np
import matplotlib.pyplot as plt
x_data=[1.0,2.0,3.0]
y_data=[2.0,4.0,6.0]

w=1.0

def forward(x):
    return x*w
    
def cost(x,y):#计算损失
    cost=0
    y_pred=forward(x)
    cost=(y_pred-y)**2
    return cost
    
def gradient(x,y):#计算梯度
    grad=2*x*(x*w-y)#数学推导得到的结果
    return grad#不取均值，对每一个样本进行求值
cost_n=[]
epoch_=[]
print(4,forward(4))
for epoch in range(100):
    for x,y in zip(x_data,y_data):
        cost_val=cost(x,y)
        grad_val=gradient(x,y)
        w-=0.01*grad_val  #学习率为0.01
        cost_n.append(cost_val)
        epoch_.append(epoch)
print(4,forward(4))
plt.plot(epoch_,cost_n)
plt.ylabel('cost')
plt.xlabel('eposch')
plt.show()
```

    4 4.0
    4 7.9999999999996945
    


    
![alt text](/assets/img/output_6_1.png)
    


**batch** 批量随机梯度下降(mini-batch)
一部分样本直接输入模型取得loss.mean()，对w进行更新

## 4 反向传播(Back Propagation)
弹性的模型结构
