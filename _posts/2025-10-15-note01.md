---
title: Pytorch深度学习实践
categories: [AI笔记]
tags: [CS,AI]
pin: false # 置顶文章
toc: true # 目录
comments: false # 评论
---

# OVERVIEW
### 深度学习
1. 机器学习以监督学习为主，深度学习是机器学习的分支。
   表示学习：提取特征--浅层与深层。其中有很多实现方式
   deeplearning：模型是神经网络，目标是表示学习
2. Classic machine learning:Input--人工特征提取--寻找y=f(x)的函数
3. **维度诅咒**：维度越多，数据需求越大--Features降维
   ![alt text](/assets/img/2025-10-15-note01-01.png)
Features(训练特征提取器)-->学习器
DEEPLEARNING:end to end
Input->训练整个模型-->output
4. 传统的机器学习策略

### 神经网络
- 仿真方式（感知机），输入后加权求解
- 反向传播--求偏导。核心：计算图
- ***原子计算符***反向传播算法！（有点晕）
  多条路径：e对b而言，就是每一条路的偏导之和，每一条路径的偏导就是每一步的偏导之积
  ![alt text](/assets/img/image.png)
- 发展
  LeNet-5-->AlexNet-->GoogleNet VGG-->ResNet
  关键是了解基本块的实现！然后进行组装
- 流行
  Theano  Tensorflow
  Caffe
  Torch

# 线性模型
- 步骤
  1. Dataset
  2. Model
  3. Training(权重训练)
  4. inferring
### 举例
#### dataset
  测试集结果已知但是暂时不能使用。
  数据采样可能不能完全模拟结果-->数据集要和真实数据集尽可能一致。
  --**过拟合**-->希望泛化能力强：对没有见过的图片的准确度也比较高
  故：训练集，开发集（评估性能），测试集
  简化：training set +test set
#### 目标：找到y=f(x)
常见思路:
- 先使用线性模型尝试，效果差再调整（Linear Model:F(x)=w*x+b）
- 如何找到最合适的w和b，简化为y_hat=w*b
   1. 随机猜测一个w,求出`|y_hat(i)-y(i)|**2=loss`
   也即loss:![alt text](/assets/img/image-1.png)
   求出loss的mean，Mean Square Error,**MSE**(平均平方误差)
   可用穷举法得到损失函数，找到最低点就是最佳w
   ```py
   import numpy as np
   import matplotlib.pyplot as plt

   x_data=[1,2,3]
   y_data=[2,4,6]

   def forward(x):#向前函数
    return x*w

   def loss(x,y):#计算损失函数
    y_pred=forward(x)
    return (y_pred-y)*(y_pred-y)
   
   w_list=[]
   mes_list=[]
   for w in np.arange(0.0,4.1,0.1):#生成从0.0到4.1间隔为0.1的数列
      print('w=',w)
      loss_sum=0
      for x_val,y_val in zip(x_data,y_data):#解包x_data和y_data配对迭代器
        y_pred_val=forward(x_val)
        loss_val=loss(x_val,y_val)
        loss_sum+=loss_val
        print('\t',x_val,y_val,y_pred_val,loss_val)

   ```

  